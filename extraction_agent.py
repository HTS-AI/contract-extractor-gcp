"""
LangGraph-based Contract Extraction Agent
Replaces traditional orchestrator with a stateful, graph-based agent workflow.
"""

import os
import re
import json
from typing import Dict, Any, Optional, Literal, Annotated, TypedDict, List
from pathlib import Path
from datetime import datetime

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool

# Local imports
from document_parser import DocumentParser


# ============== State Definition ==============

class ExtractionState(TypedDict):
    """State that flows through the extraction graph."""
    # Input
    file_path: Optional[str]
    document_text: Optional[str]
    page_map: Dict[int, str]
    use_ocr: bool
    use_gcs_vision: bool
    
    # Processing state
    document_type: Optional[str]
    classification_confidence: Optional[str]
    classification_reasoning: Optional[str]
    
    # Extraction results
    extracted_data: Dict[str, Any]
    
    # Metadata
    error: Optional[str]
    status: str  # "pending", "parsing", "classifying", "extracting", "enhancing", "completed", "failed"
    
    # Messages for agent reasoning
    messages: Annotated[List, add_messages]


# ============== Agent Tools ==============

def create_extraction_tools(api_key: str):
    """Create tools for the extraction agent."""
    
    @tool
    def classify_document(document_text: str) -> Dict[str, Any]:
        """
        Classify the document type (LEASE, NDA, or CONTRACT).
        
        Args:
            document_text: The text content of the document
            
        Returns:
            Dictionary with document_type, confidence, and reasoning
        """
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=api_key)
        
        # Truncate text if too long
        text_sample = document_text[:3000] if len(document_text) > 3000 else document_text
        
        prompt = f"""Analyze the following document and classify it as one of these four types:
1. INVOICE - A bill or invoice for goods/services (includes purchase invoices, sales invoices, tax invoices, proforma invoices, etc.)
2. LEASE - A lease agreement for property, equipment, or assets
3. NDA - A Non-Disclosure Agreement (also known as Confidentiality Agreement)
4. CONTRACT - A general contract or agreement (service agreement, employment contract, etc.)

DOCUMENT TEXT (sample):
{text_sample}

CLASSIFICATION GUIDELINES:
- If the document contains invoice number, bill number, itemized charges, tax details, GST/VAT, or payment due information â†’ classify as INVOICE
- If the document discusses rental/lease terms, lessor/lessee, rental payments â†’ classify as LEASE
- If the document is about confidentiality, non-disclosure, protecting information â†’ classify as NDA
- For other agreements, service contracts, employment contracts â†’ classify as CONTRACT

Return your response in JSON format:
{{
    "document_type": "INVOICE" | "LEASE" | "NDA" | "CONTRACT",
    "confidence": "HIGH" | "MEDIUM" | "LOW",
    "reasoning": "Brief explanation of why this classification was chosen"
}}"""
        
        response = llm.invoke([
            SystemMessage(content="You are a document classification expert. Return only valid JSON."),
            HumanMessage(content=prompt)
        ])
        
        try:
            # Try to parse JSON from response
            content = response.content
            # Handle markdown code blocks
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            result = json.loads(content.strip())
            doc_type = result.get("document_type", "CONTRACT").upper()
            if doc_type not in ["INVOICE", "LEASE", "NDA", "CONTRACT"]:
                doc_type = "CONTRACT"
            
            return {
                "document_type": doc_type,
                "confidence": result.get("confidence", "MEDIUM"),
                "reasoning": result.get("reasoning", "Document analyzed and classified")
            }
        except json.JSONDecodeError:
            # Fallback to keyword-based classification
            text_lower = document_text.lower()
            # Check for invoice keywords first
            if any(kw in text_lower for kw in ["invoice", "bill to", "invoice number", "inv no", "bill no", "tax invoice", "proforma", "gst", "vat", "subtotal", "total due", "amount due", "payment due"]):
                return {"document_type": "INVOICE", "confidence": "MEDIUM", "reasoning": "Invoice keywords detected"}
            elif any(kw in text_lower for kw in ["non-disclosure", "nondisclosure", "confidentiality agreement"]):
                return {"document_type": "NDA", "confidence": "MEDIUM", "reasoning": "NDA keywords detected"}
            elif any(kw in text_lower for kw in ["lease agreement", "lessor", "lessee", "rental"]):
                return {"document_type": "LEASE", "confidence": "MEDIUM", "reasoning": "Lease keywords detected"}
            else:
                return {"document_type": "CONTRACT", "confidence": "LOW", "reasoning": "Default classification"}
    
    @tool
    def extract_lease_data(document_text: str) -> Dict[str, Any]:
        """
        Extract data from a LEASE document.
        
        Args:
            document_text: The text content of the lease document
            
        Returns:
            Dictionary with extracted lease data
        """
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=api_key)
        
        prompt = _create_extraction_prompt("LEASE", document_text)
        
        response = llm.invoke([
            SystemMessage(content=_get_extraction_system_prompt()),
            HumanMessage(content=prompt)
        ])
        
        return _parse_extraction_response(response.content)
    
    @tool
    def extract_nda_data(document_text: str) -> Dict[str, Any]:
        """
        Extract data from an NDA document.
        
        Args:
            document_text: The text content of the NDA document
            
        Returns:
            Dictionary with extracted NDA data
        """
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=api_key)
        
        prompt = _create_extraction_prompt("NDA", document_text)
        
        response = llm.invoke([
            SystemMessage(content=_get_extraction_system_prompt()),
            HumanMessage(content=prompt)
        ])
        
        return _parse_extraction_response(response.content)
    
    @tool
    def extract_contract_data(document_text: str) -> Dict[str, Any]:
        """
        Extract data from a CONTRACT document.
        
        Args:
            document_text: The text content of the contract document
            
        Returns:
            Dictionary with extracted contract data
        """
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=api_key)
        
        prompt = _create_extraction_prompt("CONTRACT", document_text)
        
        response = llm.invoke([
            SystemMessage(content=_get_extraction_system_prompt()),
            HumanMessage(content=prompt)
        ])
        
        return _parse_extraction_response(response.content)
    
    @tool
    def extract_invoice_data(document_text: str) -> Dict[str, Any]:
        """
        Extract data from an INVOICE document (any type of invoice).
        
        Args:
            document_text: The text content of the invoice document
            
        Returns:
            Dictionary with extracted invoice data
        """
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=api_key)
        
        prompt = _create_invoice_extraction_prompt(document_text)
        
        response = llm.invoke([
            SystemMessage(content=_get_invoice_extraction_system_prompt()),
            HumanMessage(content=prompt)
        ])
        
        return _parse_extraction_response(response.content)
    
    @tool
    def calculate_risk_score(extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate risk score based on extracted data.
        
        Args:
            extracted_data: The extracted contract/invoice data
            
        Returns:
            Risk score information
        """
        risk_factors = []
        risk_score = 0
        max_risk = 100
        
        doc_type = extracted_data.get("document_type", "CONTRACT")
        
        # Missing Document Type
        if not doc_type:
            risk_score += 5
            risk_factors.append({"factor": "Missing document type", "severity": "Low", "impact": 5})
        
        # Different risk calculations for INVOICE vs other documents
        if doc_type == "INVOICE":
            # Invoice-specific risk factors
            parties = extracted_data.get("party_names", {})
            vendor = parties.get("vendor") or parties.get("party_1")
            customer = parties.get("customer") or parties.get("party_2")
            
            if not vendor and not customer:
                risk_score += 15
                risk_factors.append({"factor": "Missing vendor/customer information", "severity": "High", "impact": 15})
            
            # Check invoice number
            doc_ids = extracted_data.get("document_ids", {})
            invoice_id = doc_ids.get("invoice_id") or doc_ids.get("invoice_number")
            if not invoice_id:
                risk_score += 10
                risk_factors.append({"factor": "Missing invoice number", "severity": "Medium", "impact": 10})
            
            # Check dates
            dates = extracted_data.get("dates", {})
            invoice_date = dates.get("invoice_date") or extracted_data.get("start_date")
            if not invoice_date:
                risk_score += 10
                risk_factors.append({"factor": "Missing invoice date", "severity": "Medium", "impact": 10})
            
            due_date = dates.get("due_date") or extracted_data.get("due_date")
            if not due_date:
                risk_score += 15
                risk_factors.append({"factor": "Missing payment due date", "severity": "High", "impact": 15})
            
            # Check amounts
            amounts = extracted_data.get("amounts", {})
            total_amount = amounts.get("total") or amounts.get("amount_due") or extracted_data.get("amount")
            if not total_amount:
                risk_score += 20
                risk_factors.append({"factor": "Missing total amount", "severity": "High", "impact": 20})
            
            # Check line items
            line_items = extracted_data.get("line_items", [])
            if not line_items or len(line_items) == 0:
                risk_score += 5
                risk_factors.append({"factor": "No line items extracted", "severity": "Low", "impact": 5})
            
            # Check currency
            if not extracted_data.get("currency"):
                risk_score += 5
                risk_factors.append({"factor": "Missing currency information", "severity": "Low", "impact": 5})
            
            # Check frequency (for recurring invoices)
            if not extracted_data.get("frequency"):
                risk_score += 3
                risk_factors.append({"factor": "Missing payment frequency", "severity": "Low", "impact": 3})
        else:
            # Contract/NDA/Lease risk factors
            parties = extracted_data.get("party_names", {})
            if not parties.get("party_1") and not parties.get("party_2"):
                risk_score += 15
                risk_factors.append({"factor": "Missing party information", "severity": "High", "impact": 15})
            
            # Missing Dates
            if not extracted_data.get("start_date"):
                risk_score += 10
                risk_factors.append({"factor": "Missing start date", "severity": "High", "impact": 10})
            
            due_date_missing = not extracted_data.get("due_date")
            if due_date_missing:
                risk_score += 20
                risk_factors.append({"factor": "Missing due date", "severity": "High", "impact": 20})
            
            # Missing Payment Information
            amount_missing = not extracted_data.get("amount")
            if amount_missing:
                risk_score += 20
                risk_factors.append({"factor": "Missing payment amount", "severity": "High", "impact": 20})
            
            # Combined High Risk
            if amount_missing and due_date_missing:
                risk_score += 15
                risk_factors.append({"factor": "Missing both payment amount and due date (Critical)", "severity": "High", "impact": 15})
            
            if not extracted_data.get("frequency"):
                risk_score += 5
                risk_factors.append({"factor": "Missing payment frequency", "severity": "Low", "impact": 5})
        
        # Compliance Violations (for all document types)
        compliance_violation = extracted_data.get("rules_and_compliance_violation", "")
        if compliance_violation and compliance_violation.strip().lower() != "no violation of rules and compliance":
            risk_score += 20
            risk_factors.append({"factor": "Compliance violations detected", "severity": "High", "impact": 20})
        
        # PRIORITY CHECK: Due Date and Amount are critical attributes
        # Check if due_date and amount are missing (handle both INVOICE and other document types)
        due_date_missing = False
        amount_missing = False
        
        if doc_type == "INVOICE":
            dates = extracted_data.get("dates", {})
            due_date_missing = not (dates.get("due_date") or extracted_data.get("due_date"))
            amounts = extracted_data.get("amounts", {})
            amount_missing = not (amounts.get("total") or amounts.get("amount_due") or extracted_data.get("amount"))
        else:
            due_date_missing = not extracted_data.get("due_date")
            amount_missing = not extracted_data.get("amount")
        
        # Apply priority rules: Both missing = High risk (>=60), One missing = Medium risk (>=30)
        if due_date_missing and amount_missing:
            # Both critical attributes missing - ensure High risk minimum
            if risk_score < 60:
                risk_score = 60
                risk_factors.append({"factor": "Priority: Both due date and amount missing - High risk enforced", "severity": "High", "impact": 0})
        elif due_date_missing or amount_missing:
            # One critical attribute missing - ensure Medium risk minimum
            if risk_score < 30:
                risk_score = 30
                risk_factors.append({"factor": "Priority: One critical attribute (due date or amount) missing - Medium risk enforced", "severity": "Medium", "impact": 0})
        
        # Cap risk score
        risk_score = min(risk_score, max_risk)
        
        # Determine risk level
        if risk_score < 30:
            risk_level, risk_category = "Low", "ðŸŸ¢ Low Risk"
        elif risk_score < 60:
            risk_level, risk_category = "Medium", "ðŸŸ¡ Medium Risk"
        elif risk_score < 80:
            risk_level, risk_category = "High", "ðŸŸ  High Risk"
        else:
            risk_level, risk_category = "Critical", "ðŸ”´ Critical Risk"
        
        return {
            "score": risk_score,
            "level": risk_level,
            "category": risk_category,
            "risk_factors": risk_factors
        }
    
    return [classify_document, extract_lease_data, extract_nda_data, extract_contract_data, extract_invoice_data, calculate_risk_score]


# ============== Helper Functions ==============

def _get_extraction_system_prompt() -> str:
    """Get the system prompt for extraction."""
    return """You are an AI Contract Document Extraction Engine.
You MUST extract ALL relevant contract details with high precision and zero hallucinations.

EXTRACTION RULES:
1. If any data is missing or not present â†’ return null.
2. NEVER hallucinate or invent contract clauses.
3. Preserve original clause wording exactly.
4. Convert all dates to ISO format: YYYY-MM-DD.
5. CRITICAL: Preserve ALL decimal places in amounts - NEVER round (e.g., 12688.76 stays 12688.76, NOT 12689).
6. Return EXACT JSON â€” no explanation, no markdown, no notes.
7. CRITICAL: Extract party names from the document header, "PARTIES" section, or signature block."""


def _get_invoice_extraction_system_prompt() -> str:
    """Get the system prompt for invoice extraction."""
    return """You are an AI Invoice Data Extraction Engine.
You MUST extract ALL relevant invoice details with high precision and zero hallucinations.

EXTRACTION RULES:
1. If any data is missing or not present â†’ return null or empty string.
2. NEVER hallucinate or invent invoice details.
3. Extract exact amounts, dates, and identifiers as they appear.
4. CRITICAL: Preserve ALL decimal places in amounts - NEVER round (e.g., 12688.76 must stay 12688.76, NOT 12689).
5. Convert all dates to ISO format: YYYY-MM-DD.
6. Return EXACT JSON â€” no explanation, no markdown, no notes.
7. Extract all tax details (GST, VAT, CGST, SGST, IGST, etc.) if present.
8. Parse line items with description, quantity, rate, and amount.
9. CRITICAL: Identify vendor/supplier and customer/buyer accurately."""

def _create_extraction_prompt(doc_type: str, document_text: str) -> str:
    """Create extraction prompt for document type."""
    return f"""Analyze the following {doc_type} document and extract the following information:

DOCUMENT TEXT:
{document_text}

REQUIRED OUTPUT FORMAT (STRICT JSON):
{{
  "document_type": "{doc_type}",
  "party_names": {{
    "party_1": "",
    "party_2": "",
    "additional_parties": []
  }},
  "start_date": "",
  "due_date": "",
  "amount": "",
  "frequency": "",
  "account_type": "",
  "document_ids": {{
    "invoice_id": "",
    "contract_id": "",
    "agreement_id": "",
    "reference_id": "",
    "document_number": "",
    "lease_id": "",
    "nda_id": "",
    "po_number": "",
    "project_id": "",
    "other_ids": []
  }},
  "rules_and_compliance_violation": "",
  "risk_score": null
}}

EXTRACTION INSTRUCTIONS:
1. Document Type: Set to "{doc_type}"
2. Party Names: Extract all party names from document header, "PARTIES" section, or signature block
3. Start Date: Extract the effective/start date. Format as YYYY-MM-DD.
4. Due Date: Extract payment due date or deadline. Format as YYYY-MM-DD.
5. Amount: Extract the payment amount EXACTLY with all decimals (e.g., 12688.76 NOT 12689). Do NOT round. Do NOT extract percentages as amounts.
6. Frequency: Extract payment frequency (e.g., "Monthly", "Quarterly", "Annual")
7. Account Type: Extract account type if mentioned
8. Document IDs (Extract ALL identification numbers):
   - Look for: Contract ID, Agreement Number, Reference Number, Document Number, Lease ID, NDA Number
   - Look for: Invoice Number, PO Number, Project ID, File Number
   - Extract ANY alphanumeric identifier that identifies this document
   - Common labels: "Contract No", "Agreement #", "Ref No", "Doc #", "ID", "Number"
9. Rules and Compliance Violation: Analyze for violations. If none found, return "No violation of rules and compliance"

Return ONLY valid JSON."""


def _create_invoice_extraction_prompt(document_text: str) -> str:
    """Create extraction prompt specifically for invoices."""
    return f"""Analyze the following INVOICE document and extract ALL relevant information:

DOCUMENT TEXT:
{document_text}

REQUIRED OUTPUT FORMAT (STRICT JSON):
{{
  "document_type": "INVOICE",
  "invoice_type": "",
  "party_names": {{
    "vendor": "",
    "vendor_address": "",
    "vendor_gstin": "",
    "vendor_pan": "",
    "customer": "",
    "customer_address": "",
    "customer_gstin": "",
    "party_1": "",
    "party_2": "",
    "additional_parties": []
  }},
  "document_ids": {{
    "invoice_id": "",
    "invoice_number": "",
    "bill_number": "",
    "po_number": "",
    "order_number": "",
    "reference_id": "",
    "document_number": "",
    "quotation_number": "",
    "receipt_number": "",
    "transaction_id": "",
    "other_ids": []
  }},
  "dates": {{
    "invoice_date": "",
    "due_date": "",
    "supply_date": "",
    "delivery_date": ""
  }},
  "start_date": "",
  "due_date": "",
  "amounts": {{
    "subtotal": "",
    "discount": "",
    "taxable_amount": "",
    "cgst": "",
    "sgst": "",
    "igst": "",
    "gst": "",
    "vat": "",
    "tax_amount": "",
    "total": "",
    "amount_due": "",
    "amount_paid": "",
    "balance_due": ""
  }},
  "amount": "",
  "currency": "",
  "line_items": [
    {{
      "description": "",
      "hsn_sac_code": "",
      "quantity": "",
      "unit": "",
      "rate": "",
      "amount": "",
      "tax_rate": ""
    }}
  ],
  "payment_details": {{
    "payment_terms": "",
    "payment_method": "",
    "bank_name": "",
    "account_number": "",
    "ifsc_code": "",
    "upi_id": ""
  }},
  "frequency": "",
  "account_type": "Accounts Payable",
  "notes": "",
  "rules_and_compliance_violation": "",
  "risk_score": null
}}

EXTRACTION INSTRUCTIONS:
1. Invoice Type: Identify type (Tax Invoice, Proforma, Commercial, Construction, Lease, Service, etc.)
2. Vendor/Supplier: Extract company name issuing the invoice (look for "FROM", "BILLED BY", "VENDOR", "SUPPLIER", "SELLER", company name at top)
3. Customer/Buyer: Extract company/person being billed (look for "TO", "BILL TO", "BILLED TO", "CUSTOMER", "BUYER")
4. Document IDs (CRITICAL - Extract ALL of these if present):
   - invoice_id: Main invoice identifier (look for "INVOICE ID", "INVOICE NO", "INVOICE #", "INV NO", "INV #", "INVOICE NUMBER")
   - invoice_number: Same as invoice_id if not separately specified
   - bill_number: Bill identifier (look for "BILL NO", "BILL #", "BILL NUMBER")
   - po_number: Purchase order number (look for "PO", "P.O.", "PURCHASE ORDER", "PO NUMBER", "PO #")
   - order_number: Order reference (look for "ORDER NO", "ORDER #", "ORDER NUMBER", "ORD NO")
   - reference_id: Any reference number (look for "REF", "REF NO", "REFERENCE", "REF #")
   - document_number: Generic document number (look for "DOC NO", "DOCUMENT NO", "DOC #")
   - quotation_number: Quote reference (look for "QUOTE", "QUOTATION", "QUOTE NO", "QUO #")
   - receipt_number: Receipt identifier (look for "RECEIPT", "RECEIPT NO", "REC #")
   - transaction_id: Transaction reference (look for "TRANSACTION", "TXN", "TRANS ID")
   - other_ids: Any other identifiers found in the document
   NOTE: The primary document identifier should go in both invoice_id AND invoice_number
5. Dates: Extract invoice date, due date, delivery date as YYYY-MM-DD (be flexible with date formats)
6. Amounts and Currency: Extract ALL monetary values including taxes
   - Look for: Subtotal, Discount, Taxable Amount, Tax, CGST, SGST, IGST, GST, VAT, Total, Grand Total, Amount Due, Balance
   - CRITICAL: Extract amounts EXACTLY as shown - preserve ALL decimal places (e.g., 12688.76 NOT 12689)
   - Strip currency symbols but keep the complete numeric value with decimals
   - IMPORTANT: Detect currency from:
     * Currency symbols: $, â‚¹, â‚¬, Â£, etc.
     * Currency codes: USD, INR, EUR, GBP, AED, CAD, AUD
     * Currency words: Dollars, Rupees, Euros, Pounds, Dirhams
     * Country names: USA/India/UK/UAE (infer currency)
     * If currency symbol is missing due to OCR, look for currency mentioned ANYWHERE in the document
   - Set "currency" field with the detected currency code (USD, INR, EUR, etc.)
7. Line Items: Extract EACH product/service listed with:
   - Description/Item name
   - HSN/SAC code if present
   - Quantity
   - Unit (pcs, kg, hours, etc.)
   - Rate/Unit price
   - Amount/Total for that line
   - Tax rate (if shown per item)
8. Tax Details: Extract GST, VAT, CGST, SGST, IGST as applicable (may be shown as percentage or amount)
9. Payment Details: Extract bank details, payment terms, UPI if present
10. For party_1, use vendor name; for party_2, use customer name
11. For amount field, use the total/grand total/amount due (the final payable amount)
12. For start_date, use invoice_date; for due_date, use payment due date
13. If any field is not found in the document, leave it as empty string ""
14. Extract ALL line items, don't skip any
15. Be flexible with terminology - different invoices use different terms for the same fields

IMPORTANT NOTES:
- Construction invoices may have labor, materials, equipment as line items
- Service invoices may have service descriptions and hourly rates
- Some invoices show tax separately, some include it in total
- Invoice numbers can be in various formats: alphanumeric, with prefixes, with dashes, etc.
- Dates can be in multiple formats: DD/MM/YYYY, MM/DD/YYYY, YYYY-MM-DD, etc. - convert all to YYYY-MM-DD
- Amounts may have currency symbols ($, â‚¹, â‚¬, etc.) - extract EXACT numeric value with ALL decimals (12688.76 NOT 12689)
- NEVER round amounts - keep them exactly as shown in the document
- Look carefully at the document structure to identify vendor vs customer
- OCR ISSUES: Currency symbols ($, â‚¹) may be missing or misread - look for currency mentioned ANYWHERE in document (text, headers, footers)
- If currency symbol is missing, search for: "USD", "Dollar", "INR", "Rupee", country names, or any currency indication
- Common OCR errors: $ â†’ S, â‚¹ â†’ Rs, â‚¬ â†’ C - be flexible in detection

Return ONLY valid JSON."""


def _parse_extraction_response(content: str) -> Dict[str, Any]:
    """Parse extraction response from LLM."""
    try:
        # Handle markdown code blocks
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        return json.loads(content.strip())
    except json.JSONDecodeError:
        return {
            "document_type": "CONTRACT",
            "party_names": {"party_1": "", "party_2": "", "additional_parties": []},
            "start_date": "",
            "due_date": "",
            "amount": "",
            "frequency": "",
            "account_type": "",
            "document_ids": {},
            "rules_and_compliance_violation": "No violation of rules and compliance",
            "error": "Failed to parse extraction response"
        }


# ============== Graph Nodes ==============

def parse_document_node(state: ExtractionState) -> ExtractionState:
    """Node: Parse the document and extract text."""
    print("\n[AGENT NODE] Parsing document...")
    
    try:
        # Use the use_gcs_vision setting from state
        use_gcs = state.get("use_gcs_vision", False)
        parser = DocumentParser(use_gcs_vision=use_gcs)
        
        if state.get("file_path"):
            document_text, page_map = parser.parse_with_pages(
                state["file_path"], 
                use_ocr=state.get("use_ocr", False)
            )
            state["document_text"] = document_text
            state["page_map"] = page_map
            state["status"] = "parsing_complete"
            
            state["messages"] = [
                AIMessage(content=f"Document parsed successfully. Extracted {len(document_text)} characters from {len(page_map)} pages.")
            ]
        elif state.get("document_text"):
            state["page_map"] = {}
            state["status"] = "parsing_complete"
            state["messages"] = [
                AIMessage(content="Using provided document text directly.")
            ]
        else:
            raise ValueError("No file path or document text provided")
            
    except Exception as e:
        state["error"] = str(e)
        state["status"] = "failed"
        state["messages"] = [AIMessage(content=f"Error parsing document: {str(e)}")]
    
    return state


def classify_document_node(state: ExtractionState) -> ExtractionState:
    """Node: Classify the document type."""
    print("\n[AGENT NODE] Classifying document type...")
    
    if state.get("error"):
        return state
    
    try:
        api_key = os.getenv('OPENAI_API_KEY')
        tools = create_extraction_tools(api_key)
        classify_tool = tools[0]  # classify_document tool
        
        result = classify_tool.invoke({"document_text": state["document_text"]})
        
        state["document_type"] = result["document_type"]
        state["classification_confidence"] = result["confidence"]
        state["classification_reasoning"] = result["reasoning"]
        state["status"] = "classified"
        
        state["messages"] = [
            AIMessage(content=f"Document classified as {result['document_type']} with {result['confidence']} confidence. Reason: {result['reasoning']}")
        ]
        
        print(f"    â†’ Document Type: {result['document_type']} (Confidence: {result['confidence']})")
        
    except Exception as e:
        state["error"] = str(e)
        state["status"] = "failed"
        state["messages"] = [AIMessage(content=f"Error classifying document: {str(e)}")]
    
    return state


def extract_data_node(state: ExtractionState) -> ExtractionState:
    """Node: Extract data based on document type."""
    print(f"\n[AGENT NODE] Extracting data using {state.get('document_type', 'CONTRACT')} extractor...")
    
    if state.get("error"):
        return state
    
    try:
        api_key = os.getenv('OPENAI_API_KEY')
        tools = create_extraction_tools(api_key)
        
        doc_type = state.get("document_type", "CONTRACT")
        
        # Select the appropriate extraction tool
        # Tools order: [classify_document, extract_lease_data, extract_nda_data, extract_contract_data, extract_invoice_data, calculate_risk_score]
        if doc_type == "LEASE":
            extract_tool = tools[1]  # extract_lease_data
        elif doc_type == "NDA":
            extract_tool = tools[2]  # extract_nda_data
        elif doc_type == "INVOICE":
            extract_tool = tools[4]  # extract_invoice_data
        else:
            extract_tool = tools[3]  # extract_contract_data
        
        result = extract_tool.invoke({"document_text": state["document_text"]})
        
        state["extracted_data"] = result
        state["status"] = "extracted"
        
        # Count extracted fields
        non_empty = sum(1 for k, v in result.items() if v and v != "null" and v != {})
        
        state["messages"] = [
            AIMessage(content=f"Extracted {non_empty} fields from {doc_type} document.")
        ]
        
        print(f"    â†’ Extracted {non_empty} fields")
        
    except Exception as e:
        state["error"] = str(e)
        state["status"] = "failed"
        state["messages"] = [AIMessage(content=f"Error extracting data: {str(e)}")]
    
    return state


def enhance_data_node(state: ExtractionState) -> ExtractionState:
    """Node: Enhance extracted data with additional processing."""
    print("\n[AGENT NODE] Enhancing extracted data...")
    
    if state.get("error"):
        return state
    
    try:
        extracted_data = state.get("extracted_data", {})
        doc_type = state.get("document_type", "CONTRACT")
        
        # For invoices, flatten the nested structure for compatibility
        if doc_type == "INVOICE":
            extracted_data = _normalize_invoice_data(extracted_data)
        
        # Extract and normalize currency
        extracted_data = _extract_currency(extracted_data, state.get("document_text", ""))
        
        # Calculate per-period amount
        extracted_data = _calculate_period_amount(extracted_data)
        
        # Assign account type based on content analysis
        document_text = state.get("document_text", "")
        extracted_data = _assign_account_type(extracted_data, doc_type, document_text)
        
        state["extracted_data"] = extracted_data
        state["status"] = "enhanced"
        
        state["messages"] = [
            AIMessage(content="Data enhanced with currency normalization and period calculations.")
        ]
        
    except Exception as e:
        # Don't fail on enhancement errors, just log
        print(f"    â†’ Warning: Enhancement error: {str(e)}")
        state["messages"] = [AIMessage(content=f"Enhancement warning: {str(e)}")]
    
    return state


def calculate_risk_node(state: ExtractionState) -> ExtractionState:
    """Node: Calculate risk score."""
    print("\n[AGENT NODE] Calculating risk score...")
    
    if state.get("error"):
        return state
    
    try:
        api_key = os.getenv('OPENAI_API_KEY')
        tools = create_extraction_tools(api_key)
        # Tools order: [classify_document, extract_lease_data, extract_nda_data, extract_contract_data, extract_invoice_data, calculate_risk_score]
        risk_tool = tools[5]  # calculate_risk_score (was incorrectly set to 4, which was extract_invoice_data)
        
        result = risk_tool.invoke({"extracted_data": state.get("extracted_data", {})})
        
        state["extracted_data"]["risk_score"] = result
        state["status"] = "completed"
        
        state["messages"] = [
            AIMessage(content=f"Risk score calculated: {result['score']} ({result['level']})")
        ]
        
        print(f"    â†’ Risk Score: {result['score']} ({result['level']})")
        
    except Exception as e:
        # Don't fail on risk calculation errors
        print(f"    â†’ Warning: Risk calculation error: {str(e)}")
        state["status"] = "completed"
        state["messages"] = [AIMessage(content=f"Risk calculation warning: {str(e)}")]
    
    return state


def _find_page_references(extracted_data: Dict[str, Any], page_map: Dict[int, str]) -> Dict[str, Any]:
    """
    Find which page each extracted field came from by searching page_map.
    
    Args:
        extracted_data: The extracted data
        page_map: Dictionary mapping page numbers to page text
        
    Returns:
        Dictionary with page references for each field
    """
    if not page_map:
        return {}
    
    references = {}
    
    # Helper function to find which page contains a value
    def find_page_for_value(value, field_name=""):
        if not value or not isinstance(value, str) or len(value.strip()) == 0:
            return None
        
        # Clean value for searching
        search_value = str(value).strip()
        if len(search_value) < 2:  # Skip very short values
            return None
        
        # For dates, try multiple format variations
        if field_name in ["start_date", "due_date", "invoice_date", "delivery_date", "supply_date", "effective_date", "end_date"]:
            try:
                for page_num, page_text in page_map.items():
                    # Search for the exact date
                    if search_value in page_text:
                        return page_num
                    
                    # Try parsing the date and searching for different formats
                    if '-' in search_value:
                        parts = search_value.split('-')
                        if len(parts) == 3:
                            year, month, day = parts[0], parts[1], parts[2]
                            
                            # Try multiple date format variations
                            date_variations = [
                                f"{month}/{day}/{year}",  # MM/DD/YYYY
                                f"{day}/{month}/{year}",  # DD/MM/YYYY
                                f"{day}-{month}-{year}",  # DD-MM-YYYY
                                f"{month}-{day}-{year}",  # MM-DD-YYYY
                                f"{day}.{month}.{year}",  # DD.MM.YYYY
                                f"{month}.{day}.{year}",  # MM.DD.YYYY
                                f"{int(month)}/{int(day)}/{year}",  # M/D/YYYY (no leading zeros)
                                f"{int(day)}/{int(month)}/{year}",  # D/M/YYYY
                                year + month + day,  # YYYYMMDD
                                day + month + year,  # DDMMYYYY
                            ]
                            
                            # Remove leading zeros for more variations
                            month_no_zero = str(int(month))
                            day_no_zero = str(int(day))
                            date_variations.extend([
                                f"{month_no_zero}/{day_no_zero}/{year}",
                                f"{day_no_zero}/{month_no_zero}/{year}",
                                f"{month_no_zero}-{day_no_zero}-{year}",
                                f"{day_no_zero}-{month_no_zero}-{year}",
                            ])
                            
                            for date_var in date_variations:
                                if date_var in page_text:
                                    return page_num
            except:
                # If date parsing fails, just do standard search below
                pass
        
        # For amounts, try with and without decimals, commas, currency symbols
        if field_name == "amount":
            try:
                for page_num, page_text in page_map.items():
                    # Try exact match
                    if search_value in page_text:
                        return page_num
                    
                    # Try with comma formatting and currency symbols
                    try:
                        num_val = float(search_value.replace(',', ''))
                        
                        # Try various formats with 2 decimals
                        formatted = f"{num_val:,.2f}"
                        if formatted in page_text:
                            return page_num
                        
                        # Try with $ symbol
                        if f"${formatted}" in page_text:
                            return page_num
                        
                        # Try with rupee symbol
                        if f"â‚¹{formatted}" in page_text or f"Rs. {formatted}" in page_text:
                            return page_num
                        
                        # Try without decimals if it's a round number
                        if num_val == int(num_val):
                            formatted_int = f"{int(num_val):,}"
                            if formatted_int in page_text or f"${formatted_int}" in page_text:
                                return page_num
                        
                        # Try just the number without formatting
                        if str(num_val) in page_text:
                            return page_num
                    except:
                        pass
            except:
                pass
        
        # Standard search for all other fields
        for page_num, page_text in page_map.items():
            if search_value.lower() in page_text.lower():
                return page_num
        
        return None
    
    # Track document IDs
    doc_ids = extracted_data.get("document_ids", {})
    if isinstance(doc_ids, dict):
        for id_type, id_value in doc_ids.items():
            if id_value:
                page = find_page_for_value(id_value, id_type)
                if page:
                    references[f"document_ids_{id_type}"] = {"page": page}
    
    # Track party names
    party_names = extracted_data.get("party_names", {})
    if isinstance(party_names, dict):
        for party_key, party_value in party_names.items():
            if party_value and not party_key.endswith("_address"):  # Skip addresses
                page = find_page_for_value(party_value, party_key)
                if page:
                    references[f"party_{party_key}"] = {"page": page}
    
    # Track dates - check nested dates structure too
    dates = extracted_data.get("dates", {})
    if isinstance(dates, dict):
        if dates.get("invoice_date") and "start_date" not in references:
            page = find_page_for_value(dates["invoice_date"], "invoice_date")
            if page:
                references["start_date"] = {"page": page}
                references["invoice_date"] = {"page": page}
        if dates.get("due_date") and "due_date" not in references:
            page = find_page_for_value(dates["due_date"], "due_date")
            if page:
                references["due_date"] = {"page": page}
        if dates.get("supply_date"):
            page = find_page_for_value(dates["supply_date"], "supply_date")
            if page:
                references["supply_date"] = {"page": page}
        if dates.get("delivery_date"):
            page = find_page_for_value(dates["delivery_date"], "delivery_date")
            if page:
                references["delivery_date"] = {"page": page}
    
    # Track dates from top level
    if extracted_data.get("start_date") and "start_date" not in references:
        page = find_page_for_value(extracted_data["start_date"], "start_date")
        if page:
            references["start_date"] = {"page": page}
    
    if extracted_data.get("due_date") and "due_date" not in references:
        page = find_page_for_value(extracted_data["due_date"], "due_date")
        if page:
            references["due_date"] = {"page": page}
    
    # Track execution/effective date
    if extracted_data.get("execution_date"):
        page = find_page_for_value(extracted_data["execution_date"], "effective_date")
        if page:
            references["execution_date"] = {"page": page}
            if "start_date" not in references:
                references["start_date"] = {"page": page}
    
    if extracted_data.get("effective_date"):
        page = find_page_for_value(extracted_data["effective_date"], "effective_date")
        if page:
            references["effective_date"] = {"page": page}
    
    # Track amounts - check nested amounts structure too
    amounts = extracted_data.get("amounts", {})
    if isinstance(amounts, dict):
        # Check for total amount in nested structure
        total = amounts.get("total") or amounts.get("amount_due")
        if total:
            amount_str = str(total).replace(",", "")
            page = find_page_for_value(amount_str, "amount")
            if page:
                references["amount"] = {"page": page}
    
    # Track amounts from top level
    if extracted_data.get("amount") and "amount" not in references:
        amount_str = str(extracted_data["amount"]).replace(",", "")
        page = find_page_for_value(amount_str, "amount")
        if page:
            references["amount"] = {"page": page}
    
    # Track frequency
    if extracted_data.get("frequency"):
        page = find_page_for_value(extracted_data["frequency"], "frequency")
        if page:
            references["frequency"] = {"page": page}
        elif references.get("amount"):
            # Frequency usually near payment terms/amount
            references["frequency"] = {"page": references["amount"]["page"]}
    
    # Track currency - search more thoroughly
    if extracted_data.get("currency"):
        currency = extracted_data["currency"]
        page = None
        
        # Try finding currency code or symbol
        for page_num, page_text in page_map.items():
            # Check for currency code (USD, INR, EUR, etc.)
            if currency.upper() in page_text.upper():
                page = page_num
                break
            
            # Check for currency symbol or word
            currency_patterns = {
                "USD": ["$", "USD", "US DOLLAR", "DOLLAR"],
                "INR": ["â‚¹", "INR", "RUPEE", "Rs.", "Rs "],
                "EUR": ["â‚¬", "EUR", "EURO"],
                "GBP": ["Â£", "GBP", "POUND"],
                "AED": ["AED", "DIRHAM"]
            }
            
            if currency.upper() in currency_patterns:
                for pattern in currency_patterns[currency.upper()]:
                    if pattern in page_text.upper():
                        page = page_num
                        break
                if page:
                    break
        
        if page:
            references["currency"] = {"page": page}
        elif references.get("amount"):
            # If currency not found but amount was found, assume same page
            references["currency"] = {"page": references["amount"]["page"]}
    
    # Track invoice type
    if extracted_data.get("invoice_type"):
        page = find_page_for_value(extracted_data["invoice_type"], "invoice_type")
        if page:
            references["invoice_type"] = {"page": page}
    
    # Track account type
    if extracted_data.get("account_type"):
        page = find_page_for_value(extracted_data["account_type"], "account_type")
        if page:
            references["account_type"] = {"page": page}
    
    # Track payment details
    payment_details = extracted_data.get("payment_details", {})
    if isinstance(payment_details, dict):
        if payment_details.get("payment_terms"):
            page = find_page_for_value(payment_details["payment_terms"], "payment_terms")
            if page:
                references["payment_terms"] = {"page": page}
    
    # Track line items (find first line item description)
    line_items = extracted_data.get("line_items", [])
    if line_items and isinstance(line_items, list) and len(line_items) > 0:
        first_item = line_items[0]
        if isinstance(first_item, dict) and first_item.get("description"):
            page = find_page_for_value(first_item["description"], "line_items")
            if page:
                references["line_items"] = {"page": page}
    
    # Track governing law
    if extracted_data.get("governing_law"):
        page = find_page_for_value(extracted_data["governing_law"], "governing_law")
        if page:
            references["governing_law"] = {"page": page}
    
    # Track confidentiality clause
    if extracted_data.get("confidentiality_clause"):
        clause = extracted_data["confidentiality_clause"]
        if len(clause) > 20:  # Only search if substantial text
            page = find_page_for_value(clause[:100], "confidentiality_clause")
            if page:
                references["confidentiality_clause"] = {"page": page}
    
    # FALLBACK: If document is only 1 page, all extracted fields should reference Page 1
    if len(page_map) == 1:
        single_page = list(page_map.keys())[0]
        
        # Add Page 1 reference to critical fields that are missing it
        critical_fields = [
            "start_date", "due_date", "execution_date", "effective_date",
            "amount", "currency", "frequency"
        ]
        
        for field in critical_fields:
            if field not in references:
                # Check if the field has a value
                field_value = extracted_data.get(field)
                if not field_value:
                    # Check nested structures
                    if field in ["start_date", "due_date"]:
                        dates = extracted_data.get("dates", {})
                        if isinstance(dates, dict):
                            field_value = dates.get(field) or dates.get("invoice_date")
                    elif field == "amount":
                        amounts = extracted_data.get("amounts", {})
                        if isinstance(amounts, dict):
                            field_value = amounts.get("total") or amounts.get("amount_due")
                
                # If field has a value, assign Page 1
                if field_value:
                    references[field] = {"page": single_page}
    
    return references


def finalize_node(state: ExtractionState) -> ExtractionState:
    """Node: Finalize extraction and add metadata."""
    print("\n[AGENT NODE] Finalizing extraction...")
    
    extracted_data = state.get("extracted_data", {})
    
    # Find page references for extracted fields
    page_map = state.get("page_map", {})
    if page_map:
        print(f"    â†’ Finding page references for extracted fields...")
        print(f"    â†’ Page map has {len(page_map)} pages")
        references = _find_page_references(extracted_data, page_map)
        extracted_data["references"] = references
        print(f"    â†’ Found {len(references)} page references:")
        for key, value in references.items():
            print(f"       - {key}: Page {value.get('page', '?')}")
    
    # Add extraction metadata
    extracted_data["_extraction_metadata"] = {
        "agent_version": "langgraph-v1",
        "extraction_method": "agent_based",
        "document_type": state.get("document_type", "CONTRACT"),
        "classification_confidence": state.get("classification_confidence", "UNKNOWN"),
        "status": state.get("status", "unknown"),
        "timestamp": datetime.now().isoformat()
    }
    
    state["extracted_data"] = extracted_data
    
    if state.get("error"):
        state["status"] = "failed"
    else:
        state["status"] = "completed"
    
    print(f"\n[AGENT] Extraction completed with status: {state['status']}")
    
    return state


# ============== Enhancement Helpers ==============

def _extract_currency(extracted_data: Dict[str, Any], document_text: str) -> Dict[str, Any]:
    """Extract and normalize currency from amount and document text."""
    amount_str = extracted_data.get("amount", "")
    
    if not amount_str:
        extracted_data["currency"] = ""
        return extracted_data
    
    # Check for percentage-only values
    amount_lower = str(amount_str).lower().strip()
    is_percentage_only = bool(re.match(r'^\s*\d+\.?\d*\s*%\s*$|^\s*\d+\.?\d*\s*percent\s*$', amount_lower))
    
    if is_percentage_only:
        extracted_data["amount"] = ""
        extracted_data["currency"] = ""
        return extracted_data
    
    # Extract numeric value - keep original formatting from document
    amount_match = re.search(r'[\d,]+\.?\d*', str(amount_str))
    
    if amount_match:
        try:
            # Keep the original matched string, just remove commas for storage
            original_amount = amount_match.group()
            numeric_amount = float(original_amount.replace(',', ''))
            
            # Detect currency from multiple sources
            currency = ""
            amount_upper = str(amount_str).upper()
            doc_text_upper = document_text.upper() if document_text else ""
            
            # 1. Check amount string for currency symbols/codes
            if re.search(r'Rs\.|Rs |RS\.|RS ', amount_str, re.IGNORECASE) or "INR" in amount_upper or "â‚¹" in amount_str:
                currency = "INR"
            elif "USD" in amount_upper or "$" in amount_str or "US$" in amount_upper:
                currency = "USD"
            elif "EUR" in amount_upper or "â‚¬" in amount_str:
                currency = "EUR"
            elif "GBP" in amount_upper or "Â£" in amount_str:
                currency = "GBP"
            elif "AED" in amount_upper:
                currency = "AED"
            elif "CAD" in amount_upper or "C$" in amount_str:
                currency = "CAD"
            elif "AUD" in amount_upper or "A$" in amount_str:
                currency = "AUD"
            
            # 2. If no currency found in amount, check amounts field for nested currency
            if not currency:
                amounts = extracted_data.get("amounts", {})
                if isinstance(amounts, dict):
                    for key, value in amounts.items():
                        if value and isinstance(value, str):
                            value_upper = value.upper()
                            if "$" in value or "USD" in value_upper or "US$" in value_upper:
                                currency = "USD"
                                break
                            elif "â‚¹" in value or "INR" in value_upper or re.search(r'Rs\.', value, re.IGNORECASE):
                                currency = "INR"
                                break
                            elif "â‚¬" in value or "EUR" in value_upper:
                                currency = "EUR"
                                break
            
            # 3. If still no currency, check extracted currency field
            if not currency:
                extracted_currency = extracted_data.get("currency", "")
                if extracted_currency and extracted_currency.upper() in ["USD", "INR", "EUR", "GBP", "AED", "CAD", "AUD"]:
                    currency = extracted_currency.upper()
            
            # 4. If still no currency, search document text for currency keywords
            if not currency and document_text:
                # Check for currency mentions in document
                if any(keyword in doc_text_upper for keyword in ["USD", "US DOLLAR", "DOLLAR", "$"]):
                    currency = "USD"
                elif any(keyword in doc_text_upper for keyword in ["INR", "RUPEE", "RS.", "â‚¹"]):
                    currency = "INR"
                elif any(keyword in doc_text_upper for keyword in ["EUR", "EURO", "â‚¬"]):
                    currency = "EUR"
                elif any(keyword in doc_text_upper for keyword in ["GBP", "POUND", "Â£"]):
                    currency = "GBP"
                elif any(keyword in doc_text_upper for keyword in ["AED", "DIRHAM"]):
                    currency = "AED"
                
                # Check for country mentions to infer currency
                if not currency:
                    if any(country in doc_text_upper for country in ["INDIA", "INDIAN", "MUMBAI", "DELHI", "BANGALORE", "CHENNAI"]):
                        currency = "INR"
                    elif any(country in doc_text_upper for country in ["USA", "UNITED STATES", "AMERICA", "NEW YORK", "CALIFORNIA"]):
                        currency = "USD"
                    elif any(country in doc_text_upper for country in ["UAE", "DUBAI", "ABU DHABI"]):
                        currency = "AED"
                    elif any(country in doc_text_upper for country in ["UK", "UNITED KINGDOM", "LONDON", "BRITAIN"]):
                        currency = "GBP"
            
            # 5. Default to USD if no currency detected (common default for international invoices)
            if not currency:
                currency = "USD"
            
            # Keep the exact amount as it appears in the document (just remove commas)
            extracted_data["amount"] = original_amount.replace(',', '')
            extracted_data["currency"] = currency
            
        except (ValueError, AttributeError):
            pass
    
    return extracted_data


def _calculate_period_amount(extracted_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate per-period amount from total and frequency."""
    amount_str = extracted_data.get("amount", "")
    frequency = extracted_data.get("frequency", "")
    
    if not amount_str or not frequency:
        return extracted_data
    
    try:
        amount_match = re.search(r'[\d,]+\.?\d*', str(amount_str).replace(',', ''))
        if not amount_match:
            return extracted_data
        
        total_amount = float(amount_match.group().replace(',', ''))
        frequency_lower = str(frequency).lower().strip()
        
        # Determine periods per year
        if any(term in frequency_lower for term in ['monthly', 'month']):
            periods_per_year = 12
            period_name = "month"
        elif any(term in frequency_lower for term in ['quarterly', 'quarter']):
            periods_per_year = 4
            period_name = "quarter"
        elif any(term in frequency_lower for term in ['annual', 'yearly', 'year']):
            periods_per_year = 1
            period_name = "year"
        else:
            return extracted_data
        
        per_period_amount = total_amount / periods_per_year
        per_month_amount = total_amount / 12
        
        # Keep exact decimal values without rounding
        extracted_data["per_period_amount"] = f"{per_period_amount:.2f}"
        extracted_data["per_month_amount"] = f"{per_month_amount:.2f}"
        extracted_data["period_name"] = period_name
        
    except (ValueError, AttributeError):
        pass
    
    return extracted_data


def _classify_account_head(extracted_data: Dict[str, Any], document_text: str) -> str:
    """
    Classify account head based on document content analysis.
    Uses AI to intelligently determine the appropriate account head.
    """
    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.messages import SystemMessage, HumanMessage
        from account_heads_taxonomy import get_account_head_list
        
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            return "General Expense"
        
        # Prepare context for classification
        doc_type = extracted_data.get("document_type", "")
        invoice_type = extracted_data.get("invoice_type", "")
        
        # Get vendor/customer info
        party_names = extracted_data.get("party_names", {})
        vendor = party_names.get("vendor") or party_names.get("party_1", "")
        
        # Get line items
        line_items = extracted_data.get("line_items", [])
        line_items_text = ""
        if line_items:
            for item in line_items[:5]:  # Use first 5 items
                if isinstance(item, dict):
                    desc = item.get("description", "")
                    if desc:
                        line_items_text += f"  - {desc}\n"
        
        # Get notes/description
        notes = extracted_data.get("notes", "")
        
        # Get document excerpt (first 500 chars for context)
        doc_excerpt = document_text[:500] if document_text else ""
        
        # Create classification prompt
        prompt = f"""Analyze this invoice/document and classify it into the MOST APPROPRIATE account head category.

DOCUMENT INFORMATION:
- Document Type: {doc_type}
- Invoice Type: {invoice_type or "Not specified"}
- Vendor/Provider: {vendor or "Not specified"}
- Line Items:
{line_items_text if line_items_text else "  (No line items)"}
- Notes: {notes or "None"}

DOCUMENT EXCERPT:
{doc_excerpt}

AVAILABLE ACCOUNT HEAD CATEGORIES:
{get_account_head_list()}

CLASSIFICATION INSTRUCTIONS:
1. Analyze the document content, invoice type, line items, and vendor information
2. Select the SINGLE MOST APPROPRIATE account head from the list above
3. Consider:
   - What service/product is being billed?
   - What is the nature of the expense/revenue?
   - Industry/category keywords in descriptions
4. If no specific category matches well, use "General Expense"
5. Return ONLY the account head name, exactly as shown in the list

RETURN FORMAT:
Return only the account head name (e.g., "IT & Technical Services" or "Construction Expense")
NO explanation, NO extra text."""

        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=api_key)
        
        response = llm.invoke([
            SystemMessage(content="You are an accounting classification expert. Return only the account head name."),
            HumanMessage(content=prompt)
        ])
        
        # Extract account head from response
        account_head = response.content.strip()
        
        # Clean up response (remove quotes, extra whitespace)
        account_head = account_head.replace('"', '').replace("'", "").strip()
        
        # Validate it's in our taxonomy
        from account_heads_taxonomy import ALL_ACCOUNT_HEADS
        if account_head in ALL_ACCOUNT_HEADS.values():
            return account_head
        
        # If not exact match, try to find partial match
        for key, name in ALL_ACCOUNT_HEADS.items():
            if name.lower() in account_head.lower() or account_head.lower() in name.lower():
                return name
        
        # Default fallback
        return "General Expense"
        
    except Exception as e:
        print(f"    â†’ Warning: Account head classification error: {str(e)}")
        return "General Expense"


def _assign_account_type(extracted_data: Dict[str, Any], document_type: str, document_text: str = "") -> Dict[str, Any]:
    """Assign account type based on content analysis."""
    account_type = extracted_data.get("account_type")
    
    # If account type is already set and looks valid, keep it
    if account_type and account_type.strip() != "" and account_type.lower() not in ["null", "accounts payable", "general expense"]:
        return extracted_data
    
    # For invoices and contracts, use AI-based classification
    if document_type.upper() in ["INVOICE", "CONTRACT", "LEASE"]:
        print(f"    â†’ Classifying account head based on document content...")
        classified_account = _classify_account_head(extracted_data, document_text)
        extracted_data["account_type"] = classified_account
        print(f"    â†’ Account Head: {classified_account}")
    else:
        # For NDA and others, use simple mapping
        account_head_mapping = {
            "NDA": "Legal & Professional Fees",
            "CONTRACT": "Service Revenue"
        }
        extracted_data["account_type"] = account_head_mapping.get(document_type.upper(), "General Expense")
    
    return extracted_data


def _normalize_invoice_data(extracted_data: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize invoice data to ensure compatibility with the standard format."""
    
    # Flatten dates if nested
    dates = extracted_data.get("dates", {})
    if dates:
        if not extracted_data.get("start_date"):
            extracted_data["start_date"] = dates.get("invoice_date", "")
        if not extracted_data.get("due_date"):
            extracted_data["due_date"] = dates.get("due_date", "")
    
    # Flatten amounts if nested
    amounts = extracted_data.get("amounts", {})
    if amounts:
        # Set main amount from total or amount_due
        if not extracted_data.get("amount"):
            extracted_data["amount"] = (
                amounts.get("total") or 
                amounts.get("amount_due") or 
                amounts.get("balance_due") or
                amounts.get("taxable_amount") or
                ""
            )
        
        # Extract tax details to top level
        extracted_data["tax_details"] = {
            "subtotal": amounts.get("subtotal", ""),
            "cgst": amounts.get("cgst", ""),
            "sgst": amounts.get("sgst", ""),
            "igst": amounts.get("igst", ""),
            "gst": amounts.get("gst", ""),
            "vat": amounts.get("vat", ""),
            "tax_amount": amounts.get("tax_amount", ""),
            "discount": amounts.get("discount", "")
        }
    
    # Normalize party names for invoices
    party_names = extracted_data.get("party_names", {})
    if party_names:
        # Map vendor/customer to party_1/party_2 if not set
        if not party_names.get("party_1") and party_names.get("vendor"):
            party_names["party_1"] = party_names["vendor"]
        if not party_names.get("party_2") and party_names.get("customer"):
            party_names["party_2"] = party_names["customer"]
        extracted_data["party_names"] = party_names
    
    # Normalize document IDs - ensure primary ID is captured
    doc_ids = extracted_data.get("document_ids", {})
    if doc_ids:
        # Find the primary document identifier (try multiple fields)
        primary_id = (
            doc_ids.get("invoice_id") or 
            doc_ids.get("invoice_number") or 
            doc_ids.get("bill_number") or
            doc_ids.get("document_number") or
            doc_ids.get("reference_id") or
            ""
        )
        
        # Set both invoice_id and invoice_number to the primary ID if either is empty
        if primary_id:
            if not doc_ids.get("invoice_id"):
                doc_ids["invoice_id"] = primary_id
            if not doc_ids.get("invoice_number"):
                doc_ids["invoice_number"] = primary_id
        
        extracted_data["document_ids"] = doc_ids
    
    # Ensure currency is set - look in amounts if not in main currency field
    if not extracted_data.get("currency"):
        # Check amounts for currency information
        amounts = extracted_data.get("amounts", {})
        if amounts:
            for key, value in amounts.items():
                if value and isinstance(value, str):
                    # Look for currency symbols/codes in amount values
                    if "$" in value or "USD" in value.upper():
                        extracted_data["currency"] = "USD"
                        break
                    elif "â‚¹" in value or "INR" in value.upper() or "Rs" in value:
                        extracted_data["currency"] = "INR"
                        break
                    elif "â‚¬" in value or "EUR" in value.upper():
                        extracted_data["currency"] = "EUR"
                        break
                    elif "Â£" in value or "GBP" in value.upper():
                        extracted_data["currency"] = "GBP"
                        break
    
    return extracted_data


# ============== Conditional Edge Functions ==============

def should_continue(state: ExtractionState) -> Literal["continue", "error"]:
    """Determine if processing should continue or stop on error."""
    if state.get("error"):
        return "error"
    return "continue"


# ============== Main Agent Class ==============

class ExtractionAgent:
    """
    LangGraph-based Contract Extraction Agent.
    
    This agent uses a graph-based workflow to:
    1. Parse documents (PDF, DOCX, TXT)
    2. Classify document type (LEASE, NDA, CONTRACT)
    3. Extract relevant data using type-specific prompts
    4. Enhance data with currency/period calculations
    5. Calculate risk scores
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        use_gcs_vision: bool = True,  # Default to True - Vision API enabled for OCR
        service_account_file: Optional[str] = None,
        use_semantic_search: bool = True,  # Kept for backwards compatibility
        document_id: Optional[str] = None   # Kept for backwards compatibility
    ):
        """
        Initialize the extraction agent.
        
        Args:
            api_key: OpenAI API key (uses env var if not provided)
            use_gcs_vision: Whether to use Google Cloud Vision for scanned PDFs (default True)
            service_account_file: Path to GCP service account JSON
            use_semantic_search: Kept for backwards compatibility (agent has built-in semantic capabilities)
            document_id: Kept for backwards compatibility
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.use_gcs_vision = use_gcs_vision
        self.service_account_file = service_account_file
        self.use_semantic_search = use_semantic_search  # Stored but agent uses LLM-based extraction
        self.document_id = document_id
        
        # Build the graph
        self.graph = self._build_graph()
        self.app = self.graph.compile()
    
    def _build_graph(self) -> StateGraph:
        """Build the extraction workflow graph."""
        
        # Create the graph
        workflow = StateGraph(ExtractionState)
        
        # Add nodes
        workflow.add_node("parse", parse_document_node)
        workflow.add_node("classify", classify_document_node)
        workflow.add_node("extract", extract_data_node)
        workflow.add_node("enhance", enhance_data_node)
        workflow.add_node("risk", calculate_risk_node)
        workflow.add_node("finalize", finalize_node)
        
        # Add edges
        workflow.set_entry_point("parse")
        
        # Parse â†’ Classify (with error check)
        workflow.add_conditional_edges(
            "parse",
            should_continue,
            {
                "continue": "classify",
                "error": "finalize"
            }
        )
        
        # Classify â†’ Extract (with error check)
        workflow.add_conditional_edges(
            "classify",
            should_continue,
            {
                "continue": "extract",
                "error": "finalize"
            }
        )
        
        # Extract â†’ Enhance (with error check)
        workflow.add_conditional_edges(
            "extract",
            should_continue,
            {
                "continue": "enhance",
                "error": "finalize"
            }
        )
        
        # Enhance â†’ Risk (always continue)
        workflow.add_edge("enhance", "risk")
        
        # Risk â†’ Finalize
        workflow.add_edge("risk", "finalize")
        
        # Finalize â†’ END
        workflow.add_edge("finalize", END)
        
        return workflow
    
    def extract_from_file(
        self,
        file_path: str,
        use_ocr: bool = False
    ) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Extract data from a file.
        
        Args:
            file_path: Path to the document file
            use_ocr: Whether to use OCR for scanned PDFs
            
        Returns:
            Tuple of (extracted_data, metadata)
        """
        print("\n" + "=" * 60)
        print("[LANGGRAPH AGENT] Starting extraction workflow")
        print("=" * 60)
        print(f"  File: {Path(file_path).name}")
        
        # Initialize state
        initial_state: ExtractionState = {
            "file_path": file_path,
            "document_text": None,
            "page_map": {},
            "use_ocr": use_ocr,
            "use_gcs_vision": self.use_gcs_vision,
            "document_type": None,
            "classification_confidence": None,
            "classification_reasoning": None,
            "extracted_data": {},
            "error": None,
            "status": "pending",
            "messages": []
        }
        
        # Run the graph
        final_state = self.app.invoke(initial_state)
        
        # Extract results
        extracted_data = final_state.get("extracted_data", {})
        
        metadata = {
            "document_type": final_state.get("document_type", "CONTRACT"),
            "classification_confidence": final_state.get("classification_confidence", "UNKNOWN"),
            "classification_reasoning": final_state.get("classification_reasoning", ""),
            "file_path": file_path,
            "file_name": Path(file_path).name,
            "extraction_method": "langgraph_agent",
            "document_text": final_state.get("document_text", ""),  # Store for chatbot reuse
            "page_map": final_state.get("page_map", {}),
            "status": final_state.get("status", "unknown"),
            "error": final_state.get("error")
        }
        
        print("\n" + "=" * 60)
        print(f"[LANGGRAPH AGENT] Extraction completed: {final_state.get('status', 'unknown')}")
        print("=" * 60 + "\n")
        
        return extracted_data, metadata
    
    def extract_from_text(
        self,
        document_text: str,
        page_map: Optional[Dict[int, str]] = None
    ) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Extract data from raw text.
        
        Args:
            document_text: Raw text from document
            page_map: Optional page mapping
            
        Returns:
            Tuple of (extracted_data, metadata)
        """
        print("\n" + "=" * 60)
        print("[LANGGRAPH AGENT] Starting extraction workflow (from text)")
        print("=" * 60)
        
        # Initialize state
        initial_state: ExtractionState = {
            "file_path": None,
            "document_text": document_text,
            "page_map": page_map or {},
            "use_ocr": False,
            "use_gcs_vision": self.use_gcs_vision,
            "document_type": None,
            "classification_confidence": None,
            "classification_reasoning": None,
            "extracted_data": {},
            "error": None,
            "status": "pending",
            "messages": []
        }
        
        # Run the graph
        final_state = self.app.invoke(initial_state)
        
        # Extract results
        extracted_data = final_state.get("extracted_data", {})
        
        metadata = {
            "document_type": final_state.get("document_type", "CONTRACT"),
            "classification_confidence": final_state.get("classification_confidence", "UNKNOWN"),
            "classification_reasoning": final_state.get("classification_reasoning", ""),
            "extraction_method": "langgraph_agent",
            "document_text": final_state.get("document_text", ""),  # Store for chatbot reuse
            "page_map": final_state.get("page_map", {}),
            "status": final_state.get("status", "unknown"),
            "error": final_state.get("error")
        }
        
        print("\n" + "=" * 60)
        print(f"[LANGGRAPH AGENT] Extraction completed: {final_state.get('status', 'unknown')}")
        print("=" * 60 + "\n")
        
        return extracted_data, metadata


# ============== Backwards Compatibility ==============

# Alias for backwards compatibility with existing code
ExtractionOrchestrator = ExtractionAgent


if __name__ == "__main__":
    # Test the agent
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python extraction_agent.py <file_path>")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    agent = ExtractionAgent()
    extracted_data, metadata = agent.extract_from_file(file_path)
    
    print("\n" + "=" * 60)
    print("EXTRACTED DATA:")
    print("=" * 60)
    print(json.dumps(extracted_data, indent=2, ensure_ascii=False))

